# SIMGR-UPT: Sistema de Monitoreo y GestiÃ³n de Red para Laboratorios UPT

**DescripciÃ³n:**  
El **Sistema de Monitoreo y GestiÃ³n de Red para Laboratorios UPT (SIMGR-UPT)** tiene como objetivo supervisar y gestionar en tiempo real el rendimiento de la red en los laboratorios de la Universidad Privada de Tacna. Esto incluye monitoreo de ancho de banda, estabilidad de la conexiÃ³n y detecciÃ³n de problemas para optimizar los recursos y garantizar un entorno eficiente.



## Integrantes

| Nombre                             | CÃ³digo             |
|------------------------------------|-------------------|
| Escobar Rejas, Carlos AndrÃ©s       | (2021070016)       |
| Apaza Ccalle, Albert Kenyi         | (2021071075)       |
| Ricardo Cutipa Gutierrez           | (2021069827)       |
| Erick Churacutipa Blass            | (2020067578)       |
| Jesus Huallpa Maron                | (2021071085)       |


## Reporte de Power BI
ðŸ“Š **Puedes acceder al reporte de Power BI haciendo clic en el siguiente enlace:**

[Ver Reporte en Power BI](https://app.powerbi.com/view?r=eyJrIjoiNjMxZDkyYjQtMDFhMy00NTdkLTlkNzYtOWExODc5MGU0YmE3IiwidCI6IjE2NzFiMjY2LTJhNDktNDYyYi05Zjk1LWU4MzFjOGRlMDRkOSIsImMiOjEwfQ%3D%3D)


## Diagramas

### 1. ConstrucciÃ³n de la Infraestructura
![ConstrucciÃ³n de la Infraestructura](media/diagramaBrainboard.png)

### 2. InteracciÃ³n del Usuario sobre lo Creado
![InteracciÃ³n del Usuario](media/Brainboard-athena.png).


## Inventario de Archivos y Carpetas

### Carpeta RaÃ­z

1. **`data/`**  
   - Contiene los archivos de datos en formato CSV utilizados por el sistema.

2. **`G02_UPTRED.pbix`**  
   - Archivo de Power BI para anÃ¡lisis y visualizaciÃ³n de datos.

3. **`lambda_function.zip`**  
   - Comprimido con los siguientes componentes:
     - `s3bucket.py`: Script para subir datos al bucket S3.
     - `datos_combinados.csv`: Archivo combinado generado desde `data`.

4. **`requirements.txt`**  
   - Dependencias necesarias para los scripts Python.

5. **`s3bucket.py`**  
   - Script que sube archivos CSV al bucket S3.

6. **`sqlcsv.py`**  
   - Combina los CSV en `data` para generar `datos_combinados.csv`.

### Carpeta PruebasExpo

1. **`APPs3uploaddata.exe`**  
   - AplicaciÃ³n de escritorio en Python para subir archivos CSV al bucket S3.

2. **`final data prueba.csv`**  
   - Archivo de prueba para demostraciÃ³n.

3. **`s3uploaddata.py`**  
   - CÃ³digo fuente de `APPs3uploaddata.exe`.

### Estructura General

```
/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ [Archivos CSV...]
â”œâ”€â”€ G02_UPTRED.pbix
â”œâ”€â”€ lambda_function.zip
â”‚   â”œâ”€â”€ s3bucket.py
â”‚   â”œâ”€â”€ datos_combinados.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ s3bucket.py
â”œâ”€â”€ sqlcsv.py
â”œâ”€â”€ PruebasExpo/
â”‚   â”œâ”€â”€ APPs3uploaddata.exe
â”‚   â”œâ”€â”€ final data prueba.csv
â”‚   â”œâ”€â”€ s3uploaddata.py
```

## Pasos de AutomatizaciÃ³n de Recursos

### 1. ConfiguraciÃ³n Inicial

- AsegÃºrate de tener los siguientes archivos en el entorno local:
  - `sqlcsv.py`
  - `s3bucket.py`
  - `requirements.txt`
  - `lambda_function.zip`
  - Carpeta `data/` con los archivos CSV.

### 2. Eliminar Recursos Existentes

Si necesitas limpiar tu entorno, sigue estos pasos:

1. Eliminar datos en el bucket S3:
   ```bash
   aws s3 rm s3://netuptinteligencianegocios --recursive
   ```

2. Eliminar crawler en AWS Glue:
   ```bash
   aws glue delete-crawler --name netuptinteligencianegocios-crawler
   ```

3. Destruir la infraestructura de AWS con Terraform:
   ```bash
   terraform destroy --auto-approve
   ```

### 3. ConfiguraciÃ³n y GeneraciÃ³n de Recursos

1. UbÃ­cate en la carpeta que contiene `main.tf` (Terraform).

2. Inicia Terraform:
   ```bash
   terraform init
   ```

3. Crea la infraestructura:
   ```bash
   terraform apply --auto-approve
   ```

### 4. AutomatizaciÃ³n con Scripts

1. Combina los archivos CSV de `data/` con `sqlcsv.py`:
   ```bash
   python3 sqlcsv.py
   ```

2. Sube el archivo combinado al bucket S3 con `s3bucket.py`:
   ```bash
   python3 s3bucket.py
   ```

## ConfiguraciÃ³n del Controlador ODBC para Power BI

### 1. Descargar e Instalar Controlador

- Descarga el controlador desde:  
  [Amazon Athena ODBC Driver v2.0.3.0](https://downloads.athena.us-east-1.amazonaws.com/drivers/ODBC/v2.0.3.0/Windows/AmazonAthenaODBC-2.0.3.0.msi).

### 2. ConfiguraciÃ³n del ODBC

- Abre el Administrador de ODBC (64 bits).
- Configura un nuevo DSN con los siguientes parÃ¡metros:
  - **Data Source Name**: `athena-in`
  - **Description**: `athena-in in Power BI`
  - **Region**: `us-east-1`
  - **Catalog**: `AwsDataCatalog`
  - **Database**: `default`
  - **Workgroup**: `primary`
  - **S3 Output**: `s3://netuptinteligencianegocios/athena-output/`
  - **Authentication Options**: `DEFAULT CREDENTIALS`

### 3. Probar la ConexiÃ³n

- Presiona **Test** en el ODBC configurado.
- Resultado esperado:
  ```
  Successfully connected to: Athena engine version 3
  Region: us-east-1
  Catalog: AwsDataCatalog
  Workgroup: primary
  Auth Type: Default Credentials
  ```

## Carga de Datos en Power BI

1. Abre Power BI y selecciona **Obtener Datos â†’ Amazon Athena**.
2. Configura el DSN:
   - Introduce el DSN configurado: `athena-in`.
3. Selecciona las tablas disponibles desde `AwsDataCatalog` y cÃ¡rgalas en Power BI.
4. Realiza ajustes en las visualizaciones segÃºn sea necesario.

## Notas Finales

- AsegÃºrate de que las credenciales de AWS estÃ¡n configuradas antes de ejecutar los scripts.
- Antes de cargar datos en Power BI, verifica que el bucket S3 contiene los datos esperados.
- Durante la exposiciÃ³n, utiliza `final data prueba.csv` para las pruebas de carga en tiempo real.
